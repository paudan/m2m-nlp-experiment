{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb phrase/noun phrase extraction performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv(os.path.join('..', 'datasets', 'activity_dataset_final.csv'), sep=';')\n",
    "# Exlude cases which contain multiple verb phrase cases \n",
    "# Such cases might come from disjunctive/conjunctiove clauses\n",
    "\n",
    "def single_verb(x):\n",
    "    if pd.isnull(x):\n",
    "        return True\n",
    "    return len(list(itertools.chain([v.split('|') for v in x.split(';')]))) == 1\n",
    "    \n",
    "ind_original = original['Verb phrases'].apply(single_verb)\n",
    "original = original[ind_original]\n",
    "ind_original_vp = ~pd.isnull(original['Verb phrases'])\n",
    "original['VerbRequired'] = ind_original_vp.astype(int)\n",
    "original['NounRequired'] = (~pd.isnull(original['Noun phrases'])).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "      <th>VerbPhrases</th>\n",
       "      <th>NounPhrases</th>\n",
       "      <th>ProperNouns</th>\n",
       "      <th>VerbRequired</th>\n",
       "      <th>NounRequired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User</td>\n",
       "      <td>Open Enclosure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Open Enclosure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liquid Cooling Module</td>\n",
       "      <td>Monitor LCM Status</td>\n",
       "      <td>Monitors</td>\n",
       "      <td>Status</td>\n",
       "      <td>LCM</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRS</td>\n",
       "      <td>Periodic Set Reference Temperature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Periodic Set Reference Temperature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User</td>\n",
       "      <td>Get M10 Tilt Angles</td>\n",
       "      <td>Gets</td>\n",
       "      <td>Angles</td>\n",
       "      <td>M10 Tilt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Translation Stage Drive</td>\n",
       "      <td>Preset FSS Translation Stage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Preset FSS Translation Stage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Subject                            Activity VerbPhrases  \\\n",
       "0                     User                      Open Enclosure         NaN   \n",
       "1    Liquid Cooling Module                  Monitor LCM Status    Monitors   \n",
       "2                      TRS  Periodic Set Reference Temperature         NaN   \n",
       "3                     User                 Get M10 Tilt Angles        Gets   \n",
       "4  Translation Stage Drive        Preset FSS Translation Stage         NaN   \n",
       "\n",
       "                          NounPhrases ProperNouns  VerbRequired  NounRequired  \n",
       "0                      Open Enclosure         NaN             0             1  \n",
       "1                              Status         LCM             1             1  \n",
       "2  Periodic Set Reference Temperature         NaN             0             1  \n",
       "3                              Angles    M10 Tilt             1             1  \n",
       "4        Preset FSS Translation Stage         NaN             0             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stanza = pd.read_csv('stanza-phrases.csv',sep=';')\n",
    "df_stanza = df_stanza[ind_original]\n",
    "df_stanza['VerbRequired'] = (~pd.isnull(df_stanza['VerbPhrases'])).astype(int)\n",
    "df_stanza['NounRequired'] = (~pd.isnull(df_stanza['NounPhrases'])).astype(int)\n",
    "df_stanza.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of detection if verb phrase is extracted when required for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 106   10]\n",
      " [ 349 1506]]\n",
      "Accuracy: 0.8178589548452562\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original['VerbRequired'], df_stanza['VerbRequired']))\n",
    "print('Accuracy:', accuracy_score(original['VerbRequired'], df_stanza['VerbRequired']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of detection if noun phrase is extracted when required for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  27    6]\n",
      " [  44 1894]]\n",
      "Accuracy: 0.9746321664129883\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original['NounRequired'], df_stanza['NounRequired']))\n",
    "print('Accuracy:', accuracy_score(original['NounRequired'], df_stanza['NounRequired']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate matching statistics for both cases when transformation output is repersented one or more noun phrases, or by a \"verb phrase-noun phrase tuple\". For the latter case, it is important to calculate errors for both extraction of verb phrase and noun phrase parts. If the output is represented by noun phrase, verb phrase entry in the original dataset is empty. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_verbs(verb1, verb2):\n",
    "    if pd.isnull(verb1) & pd.isnull(verb2): return True\n",
    "    if (~pd.isnull(verb1) and pd.isnull(verb2)) or (pd.isnull(verb1) and ~pd.isnull(verb2)): return False\n",
    "    return verb1.lower() == verb2.lower()\n",
    "\n",
    "def equal_outputs(out1, out2):\n",
    "    if pd.isnull(out1) & pd.isnull(out2): return True\n",
    "    if (~pd.isnull(out1) & pd.isnull(out2)) | (pd.isnull(out1) & ~pd.isnull(out2)): return False\n",
    "    out1 = out1.split('|')\n",
    "    out2 = out2.split('|')\n",
    "    return sorted(list(map(lambda x: x.lower(), out1))) == sorted(list(map(lambda x: x.lower(), out2)))\n",
    "\n",
    "# Extract matching statistics for cases when transformation output will be noun\n",
    "# No need to check for verb extraction statistics\n",
    "stanza_matches_outputn = pd.Series(map(lambda x, y: equal_outputs(x, y), original[~ind_original_vp]['Noun phrases'], df_stanza[~ind_original_vp]['NounPhrases']), \n",
    "                                   index=df_stanza[~ind_original_vp].index)\n",
    "\n",
    "# Extract matching statistics for cases when transformation output will be as \"verb-noun\" type (e.g. association-class)\n",
    "stanza_matches_outputv_np = pd.Series(map(lambda x, y: equal_outputs(x, y), original[ind_original_vp]['Noun phrases'], df_stanza[ind_original_vp]['NounPhrases']),\n",
    "                                     index=df_stanza[ind_original_vp].index)\n",
    "stanza_matches_outputv_vp = pd.Series(map(lambda x, y: equal_verbs(x, y), original[ind_original_vp]['Verb phrases'], df_stanza[ind_original_vp]['VerbPhrases']),\n",
    "                                     index=df_stanza[ind_original_vp].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun extraction matching precision for transformation cases when output will be noun: 0.831858407079646\n",
      "Verb extraction matching precision for transformation cases when output will be noun: nan\n",
      "Noun extraction matching precision for transformation cases when output will be noun: 0.7604924454392837\n",
      "Verb extraction matching precision for transformation cases when output will be noun: 0.6233911583659765\n"
     ]
    }
   ],
   "source": [
    "stanza_matches_outputn_nenp = stanza_matches_outputn[~pd.isnull(df_stanza[~ind_original_vp]['NounPhrases'])]\n",
    "prec_outputn = sum(stanza_matches_outputn_nenp)/len(stanza_matches_outputn_nenp)\n",
    "print('Noun extraction matching precision for transformation cases when output will be noun:', prec_outputn)\n",
    "# No need to calculate precision for verb phrase part, as output will be only noun\n",
    "print('Verb extraction matching precision for transformation cases when output will be noun:', np.nan)\n",
    "stanza_matches_outputv_ne = ~pd.isnull(df_stanza[ind_original_vp]['NounPhrases'])\n",
    "stanza_matches_outputv_nenp = stanza_matches_outputv_np[stanza_matches_outputv_ne]\n",
    "prec_outputv_np = sum(stanza_matches_outputv_nenp)/len(stanza_matches_outputv_nenp)\n",
    "print('Noun extraction matching precision for transformation cases when output will be noun:', prec_outputv_np)\n",
    "stanza_matches_outputv_nevp = stanza_matches_outputv_vp[stanza_matches_outputv_ne]\n",
    "prec_outputv_vp = sum(stanza_matches_outputv_nevp)/len(stanza_matches_outputv_nevp)  \n",
    "print('Verb extraction matching precision for transformation cases when output will be noun:', prec_outputv_vp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun extraction matching recall for transformation cases when output will be noun: 0.8103448275862069\n",
      "Verb extraction matching recall for transformation cases when output will be noun: nan\n",
      "Noun extraction matching recall for transformation cases when output will be noun: 0.7471698113207547\n",
      "Verb extraction matching recall for transformation cases when output will be noun: 0.6215633423180593\n"
     ]
    }
   ],
   "source": [
    "recall_outputn = sum(stanza_matches_outputn)/len(stanza_matches_outputn)\n",
    "print('Noun extraction matching recall for transformation cases when output will be noun:', recall_outputn)\n",
    "print('Verb extraction matching recall for transformation cases when output will be noun:', np.nan)\n",
    "recall_outputv_np = sum(stanza_matches_outputv_np)/len(stanza_matches_outputv_np)\n",
    "print('Noun extraction matching recall for transformation cases when output will be noun:', recall_outputv_np)\n",
    "recall_outputv_vp = sum(stanza_matches_outputv_vp)/len(stanza_matches_outputv_vp)\n",
    "print('Verb extraction matching recall for transformation cases when output will be noun:', recall_outputv_vp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun extraction matching F1 score for transformation cases when output will be noun: 0.8209606986899564\n",
      "Verb extraction matching F1 score for transformation cases when output will be noun: nan\n",
      "Noun extraction matching F1 score for transformation cases when output will be noun: 0.7537722648716106\n",
      "Verb extraction matching F1 score for transformation cases when output will be noun: 0.6224759085614535\n"
     ]
    }
   ],
   "source": [
    "def f1_score(precision, recall):\n",
    "    return 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('Noun extraction matching F1 score for transformation cases when output will be noun:', f1_score(prec_outputn, recall_outputn))\n",
    "print('Verb extraction matching F1 score for transformation cases when output will be noun:', np.nan)\n",
    "print('Noun extraction matching F1 score for transformation cases when output will be noun:', f1_score(prec_outputv_np, recall_outputv_np))\n",
    "print('Verb extraction matching F1 score for transformation cases when output will be noun:', f1_score(prec_outputv_vp, recall_outputv_vp)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition performance evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_original = pd.read_csv(os.path.join('..', 'datasets', 'ner_dataset_final.csv'), sep=';')\n",
    "ner_stanza = pd.read_csv('stanza-ner.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out extracted entities which are not PERSON, LOCATION or ORGANIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_other_ner(row):\n",
    "    x = row\n",
    "    if pd.isnull(x['Entities']):\n",
    "        return x\n",
    "    entities = x['Entities'].split('|')\n",
    "    types = x['EntityType'].split('|')\n",
    "    entities = [x for ind, x in enumerate(entities) if types[ind] in ['PERSON', 'ORGANIZATION', 'LOCATION']]\n",
    "    types = [x for x in types if x in ['PERSON', 'ORGANIZATION', 'LOCATION']]\n",
    "    x['Entities'] = '|'.join(entities)\n",
    "    x['EntityType'] = '|'.join(types)\n",
    "    if len(x['Entities']) == 0: x['Entities'] = None\n",
    "    if len(x['EntityType']) == 0: x['EntityType'] = None\n",
    "    return x\n",
    "    \n",
    "ner_stanza = ner_stanza.apply(remove_other_ner, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect if entity is extracted when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1534   60]\n",
      " [ 102  348]]\n",
      "Accuracy: 0.9207436399217221\n"
     ]
    }
   ],
   "source": [
    "ind_has_entity = ~pd.isnull(ner_original['Entities'])\n",
    "ner_original['HasEntity'] = ind_has_entity.astype(int)\n",
    "ner_stanza['HasEntity'] = (~pd.isnull(ner_stanza['Entities'])).astype(int)\n",
    "print(confusion_matrix(ner_original['HasEntity'], ner_stanza['HasEntity']))\n",
    "print('Accuracy:', accuracy_score(ner_original['HasEntity'], ner_stanza['HasEntity']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance if valid named entities are extracted (at token level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER precision: 0.9396551724137931\n",
      "NER recall: 0.7266666666666667\n",
      "NER F1: 0.8195488721804512\n"
     ]
    }
   ],
   "source": [
    "ner_matches = pd.Series(map(lambda x, y: equal_outputs(x, y), ner_original[ind_has_entity]['Entities'], ner_stanza[ind_has_entity]['Entities']),\n",
    "                        index=ner_stanza[ind_has_entity].index)\n",
    "stanza_matches_ne_token = ner_matches[~pd.isnull(ner_stanza[ind_has_entity]['Entities'])]\n",
    "prec_ner_token = sum(stanza_matches_ne_token)/len(stanza_matches_ne_token)\n",
    "print('NER precision:', prec_ner_token)\n",
    "recall_ner_token = sum(ner_matches)/len(ner_matches)\n",
    "print('NER recall:', recall_ner_token)\n",
    "print('NER F1:', f1_score(prec_ner_token, recall_ner_token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance if valid named entity types are extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER precision: 0.9425287356321839\n",
      "NER recall: 0.7311111111111112\n",
      "NER F1: 0.8234665689910412\n"
     ]
    }
   ],
   "source": [
    "ner_matches = pd.Series(map(lambda x, y: equal_outputs(x, y), ner_original[ind_has_entity]['EntityType'], ner_stanza[ind_has_entity]['EntityType']),\n",
    "                        index=ner_stanza[ind_has_entity].index)\n",
    "stanza_matches_ne_token = ner_matches[~pd.isnull(ner_stanza[ind_has_entity]['EntityType'])]\n",
    "prec_ner_token = sum(stanza_matches_ne_token)/len(stanza_matches_ne_token)\n",
    "print('NER precision:', prec_ner_token)\n",
    "recall_ner_token = sum(ner_matches)/len(ner_matches)\n",
    "print('NER recall:', recall_ner_token)\n",
    "print('NER F1:', f1_score(prec_ner_token, recall_ner_token))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
